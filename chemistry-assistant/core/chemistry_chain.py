#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŒ–å­¦åˆ†æé“¾
åŸºäºLangChainå®ç°çš„åŒ–å­¦é—®é¢˜åˆ†æå·¥ä½œæµ
"""

import logging
from typing import Dict, Any, List, Tuple
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import HumanMessage, SystemMessage
from .llm_manager import LLMManager
from tools.rag_retriever import RAGRetriever

class ChemistryAnalysisChain:
    """
    åŒ–å­¦åˆ†æé“¾ç±»
    å®ç°åŒ–å­¦é—®é¢˜çš„é“¾å¼åˆ†æå¤„ç†
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–åŒ–å­¦åˆ†æé“¾
        """
        self.logger = logging.getLogger(__name__)
        self.llm_manager = LLMManager()
        self.rag_retriever = RAGRetriever()
        self._setup_chains()
    
    def _create_rag_chain(self):
        """
        åˆ›å»ºRAGæ£€ç´¢é“¾
        """
        retriever = self.rag_retriever.get_retriever(db_name='textbooks')
        if not retriever:
            return None

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | self.rag_prompt
            | self.llm_manager.get_model('zhipu')
            | StrOutputParser()
        )
        return rag_chain

    def _setup_chains(self):
        """
        è®¾ç½®åˆ†æé“¾
        """
        self._rag_chain = self._create_rag_chain()

        # é—®é¢˜åˆ†ç±»æç¤ºæ¨¡æ¿
        self.classification_prompt = PromptTemplate(
            input_variables=["question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªåŒ–å­¦æ•™è‚²ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹åŒ–å­¦é—®é¢˜çš„ç±»å‹å’Œéš¾åº¦çº§åˆ«ã€‚

é—®é¢˜: {question}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
ç±»å‹: [æœ‰æœºåŒ–å­¦/æ— æœºåŒ–å­¦/ç‰©ç†åŒ–å­¦/åˆ†æåŒ–å­¦/ç”Ÿç‰©åŒ–å­¦]
éš¾åº¦: [åŸºç¡€/ä¸­ç­‰/å›°éš¾]
å…³é”®æ¦‚å¿µ: [åˆ—å‡º3-5ä¸ªç›¸å…³çš„åŒ–å­¦æ¦‚å¿µ]
è§£é¢˜ç­–ç•¥: [ç®€è¿°è§£é¢˜æ€è·¯]
"""
        )
        
        # å¤šè§’åº¦åˆ†ææç¤ºæ¨¡æ¿
        self.analysis_prompt = PromptTemplate(
            input_variables=["question", "classification"],
            template="""
åŸºäºé—®é¢˜åˆ†ç±»ä¿¡æ¯ï¼Œè¯·ä»å¤šä¸ªè§’åº¦åˆ†æè¿™ä¸ªåŒ–å­¦é—®é¢˜ï¼š

é—®é¢˜: {question}

åˆ†ç±»ä¿¡æ¯: {classification}

è¯·æä¾›ï¼š
1. ç†è®ºåŸºç¡€åˆ†æ
2. å®éªŒè§’åº¦åˆ†æï¼ˆå¦‚é€‚ç”¨ï¼‰
3. è®¡ç®—æ–¹æ³•åˆ†æï¼ˆå¦‚é€‚ç”¨ï¼‰
4. å®é™…åº”ç”¨è”ç³»
5. å¸¸è§é”™è¯¯æé†’

è¯·ç¡®ä¿åˆ†æå…¨é¢ä¸”å‡†ç¡®ã€‚
"""
        )
        
        # RAG æ£€ç´¢åŠ å¼ºæç¤ºæ¨¡æ¿
        self.rag_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªåŒ–å­¦ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜â€œçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯â€ã€‚

èƒŒæ™¯çŸ¥è¯†:
{context}

é—®é¢˜: {question}
"""
        )

        # è§£ç­”ç”Ÿæˆæç¤ºæ¨¡æ¿
        self.solution_prompt = PromptTemplate(
            input_variables=["question", "classification", "analysis"],
            template="""
åŸºäºå‰é¢çš„åˆ†ç±»å’Œåˆ†æï¼Œè¯·ç”Ÿæˆè¿™ä¸ªåŒ–å­¦é—®é¢˜çš„å®Œæ•´è§£ç­”ï¼š

é—®é¢˜: {question}

åˆ†ç±»ä¿¡æ¯: {classification}

å¤šè§’åº¦åˆ†æ: {analysis}

è¯·æä¾›ï¼š
1. è¯¦ç»†çš„è§£é¢˜æ­¥éª¤
2. å¿…è¦çš„åŒ–å­¦æ–¹ç¨‹å¼ï¼ˆä½¿ç”¨LaTeXæ ¼å¼ï¼‰
3. è®¡ç®—è¿‡ç¨‹ï¼ˆå¦‚é€‚ç”¨ï¼‰
4. æœ€ç»ˆç­”æ¡ˆ
5. è§£é¢˜è¦ç‚¹æ€»ç»“

ç¡®ä¿è§£ç­”å‡†ç¡®ã€å®Œæ•´ã€æ˜“æ‡‚ã€‚
"""
        )
    
    def classify_question(self, question: str) -> str:
        """
        åˆ†ç±»åŒ–å­¦é—®é¢˜
        
        Args:
            question: åŒ–å­¦é—®é¢˜
            
        Returns:
            str: åˆ†ç±»ç»“æœ
        """
        try:
            # é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹è¿›è¡Œåˆ†ç±»
            model_name = self._select_best_model(['zhipu', 'openai', 'tongyi'])
            if not model_name:
                return "æ— å¯ç”¨æ¨¡å‹è¿›è¡Œé—®é¢˜åˆ†ç±»"
            
            prompt = self.classification_prompt.format(question=question)
            messages = [HumanMessage(content=prompt)]
            
            return self.llm_manager.call_model(model_name, messages)
            
        except Exception as e:
            self.logger.error(f"é—®é¢˜åˆ†ç±»å¤±è´¥: {str(e)}")
            return f"åˆ†ç±»å¤±è´¥: {str(e)}"

    def invoke_rag_chain(self, question: str) -> str:
        """
        ç›´æ¥è°ƒç”¨RAGé“¾è¿›è¡Œé—®ç­”
        """
        if not self._rag_chain:
            return "RAGåŠŸèƒ½æœªåˆå§‹åŒ–ã€‚"
        return self._rag_chain.invoke(question)
    
    def analyze_question(self, question: str, classification: str) -> str:
        """
        å¤šè§’åº¦åˆ†æé—®é¢˜
        
        Args:
            question: åŒ–å­¦é—®é¢˜
            classification: åˆ†ç±»ç»“æœ
            
        Returns:
            str: åˆ†æç»“æœ
        """
        try:
            model_name = self._select_best_model(['zhipu', 'openai', 'tongyi'])
            if not model_name:
                return "æ— å¯ç”¨æ¨¡å‹è¿›è¡Œé—®é¢˜åˆ†æ"
            
            prompt = self.analysis_prompt.format(
                question=question,
                classification=classification
            )
            messages = [HumanMessage(content=prompt)]
            
            return self.llm_manager.call_model(model_name, messages)
            
        except Exception as e:
            self.logger.error(f"é—®é¢˜åˆ†æå¤±è´¥: {str(e)}")
            return f"åˆ†æå¤±è´¥: {str(e)}"
    
    def generate_solution(self, question: str, classification: str, analysis: str) -> str:
        """
        ç”Ÿæˆè§£ç­”
        
        Args:
            question: åŒ–å­¦é—®é¢˜
            classification: åˆ†ç±»ç»“æœ
            analysis: åˆ†æç»“æœ
            
        Returns:
            str: å®Œæ•´è§£ç­”
        """
        try:
            model_name = self._select_best_model(['zhipu', 'openai', 'tongyi'])
            if not model_name:
                return "æ— å¯ç”¨æ¨¡å‹ç”Ÿæˆè§£ç­”"
            
            prompt = self.solution_prompt.format(
                question=question,
                classification=classification,
                analysis=analysis
            )
            messages = [HumanMessage(content=prompt)]
            
            return self.llm_manager.call_model(model_name, messages, temperature=0.3)
            
        except Exception as e:
            self.logger.error(f"è§£ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"è§£ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def process_question_chain(self, question: str) -> Dict[str, str]:
        """
        é“¾å¼å¤„ç†åŒ–å­¦é—®é¢˜
        
        Args:
            question: åŒ–å­¦é—®é¢˜
            
        Returns:
            Dict[str, str]: åŒ…å«å„é˜¶æ®µç»“æœçš„å­—å…¸
        """
        # é¦–å…ˆè¿›è¡ŒRAGæ£€ç´¢
        if self._rag_chain:
            self.logger.info("[åŒ–å­¦åˆ†æé“¾] æ‰§è¡ŒRAGæ£€ç´¢...")
            rag_result = self._rag_chain.invoke(question)
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] RAGç»“æœ: {rag_result[:100]}...")
            # å°†RAGç»“æœä½œä¸ºé—®é¢˜çš„ä¸€éƒ¨åˆ†ï¼Œæˆ–ä¸Šä¸‹æ–‡
            question = f"èƒŒæ™¯çŸ¥è¯†: {rag_result}\n\né—®é¢˜: {question}"


        try:
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] å¼€å§‹é“¾å¼å¤„ç†é—®é¢˜: {question[:50]}...")
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] é—®é¢˜é•¿åº¦: {len(question)}")
            

            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜åˆ†ç±»
            self.logger.info("[åŒ–å­¦åˆ†æé“¾] æ­¥éª¤1: é—®é¢˜åˆ†ç±»")
            classification = self.classify_question(question)
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] åˆ†ç±»ç»“æœé•¿åº¦: {len(classification)}")
            
            # ç¬¬äºŒæ­¥ï¼šå¤šè§’åº¦åˆ†æ
            self.logger.info("[åŒ–å­¦åˆ†æé“¾] æ­¥éª¤2: å¤šè§’åº¦åˆ†æ")
            analysis = self.analyze_question(question, classification)
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] åˆ†æç»“æœé•¿åº¦: {len(analysis)}")
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆè§£ç­”
            self.logger.info("[åŒ–å­¦åˆ†æé“¾] æ­¥éª¤3: ç”Ÿæˆè§£ç­”")
            solution = self.generate_solution(question, classification, analysis)
            self.logger.info(f"[åŒ–å­¦åˆ†æé“¾] è§£ç­”ç»“æœé•¿åº¦: {len(solution)}")
            
            result = {
                'question': question,
                'classification': classification,
                'analysis': analysis,
                'solution': solution,
                'chain_summary': self._generate_chain_summary(classification, analysis, solution)
            }
            
            self.logger.info("[åŒ–å­¦åˆ†æé“¾] é“¾å¼å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"[åŒ–å­¦åˆ†æé“¾] é“¾å¼å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"[åŒ–å­¦åˆ†æé“¾] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return {
                'question': question,
                'error': f"å¤„ç†å¤±è´¥: {str(e)}",
                'classification': '',
                'analysis': '',
                'solution': '',
                'chain_summary': ''
            }
    
    def _select_best_model(self, preferred_models: List[str]) -> str:
        """
        é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹
        
        Args:
            preferred_models: ä¼˜å…ˆæ¨¡å‹åˆ—è¡¨
            
        Returns:
            str: é€‰ä¸­çš„æ¨¡å‹åç§°
        """
        return "default"
    
    def _generate_chain_summary(self, classification: str, analysis: str, solution: str) -> str:
        """
        ç”Ÿæˆé“¾å¼å¤„ç†æ‘˜è¦
        
        Args:
            classification: åˆ†ç±»ç»“æœ
            analysis: åˆ†æç»“æœ
            solution: è§£ç­”ç»“æœ
            
        Returns:
            str: å¤„ç†æ‘˜è¦
        """
        # æ¸…ç†è¾“å…¥å†…å®¹çš„ç¼–ç é—®é¢˜
        def clean_text(text):
            if not isinstance(text, str):
                text = str(text)
            # ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œä¹±ç 
            import re
            text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
            # ç¡®ä¿UTF-8ç¼–ç æ­£ç¡®
            try:
                text = text.encode('utf-8').decode('utf-8')
            except UnicodeError:
                text = ''.join(char for char in text if ord(char) < 65536)
            return text
        
        classification = clean_text(classification)
        analysis = clean_text(analysis)
        solution = clean_text(solution)
        
        # å¤„ç†LaTeXå…¬å¼æ ¼å¼
        def format_latex(text):
            if not text:
                return text
            
            # å°†\text{}æ ¼å¼è½¬æ¢ä¸ºæ™®é€šæ–‡æœ¬
            import re
            text = re.sub(r'\\text\{([^}]+)\}', r'\1', text)
            
            # ç¡®ä¿åŒ–å­¦æ–¹ç¨‹å¼ä½¿ç”¨æ­£ç¡®çš„LaTeXæ ¼å¼
            # å°†åŒ–å­¦å…ƒç´ ç¬¦å·åŒ…è£…åœ¨\ce{}ä¸­
            text = re.sub(r'([A-Z][a-z]?[0-9]*(?:_[0-9]+)?(?:\^[+-]?[0-9]*)?)', r'$\\ce{\1}$', text)
            
            # å¤„ç†åŒ–å­¦æ–¹ç¨‹å¼ä¸­çš„ç®­å¤´
            text = re.sub(r'\\rightarrow', r'$\\rightarrow$', text)
            text = re.sub(r'â†’', r'$\\rightarrow$', text)
            
            # å¤„ç†æ•°å­¦è¡¨è¾¾å¼
            text = re.sub(r'\\([a-zA-Z]+)', r'$\\\1$', text)
            
            return text
        
        formatted_classification = format_latex(classification)
        formatted_analysis = format_latex(analysis)
        formatted_solution = format_latex(solution)
        
        return f"""
### ğŸ”¬ åŒ–å­¦é—®é¢˜é“¾å¼åˆ†ææŠ¥å‘Š

**ğŸ“‹ é—®é¢˜åˆ†ç±»**
{formatted_classification}

**ğŸ” å¤šè§’åº¦åˆ†æ**
{formatted_analysis}

**âœ… å®Œæ•´è§£ç­”**
{formatted_solution}

---
*æœ¬æŠ¥å‘Šç”±LangChainåŒ–å­¦åˆ†æé“¾è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    def get_chain_info(self) -> Dict[str, Any]:
        """
        è·å–åˆ†æé“¾ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: é“¾ä¿¡æ¯
        """
        return {
            'name': 'åŒ–å­¦åˆ†æé“¾',
            'description': 'åŸºäºLangChainçš„åŒ–å­¦é—®é¢˜é“¾å¼åˆ†æå·¥å…·',
            'steps': ['é—®é¢˜åˆ†ç±»', 'å¤šè§’åº¦åˆ†æ', 'è§£ç­”ç”Ÿæˆ'],

            'features': [
                'è‡ªåŠ¨é—®é¢˜åˆ†ç±»',
                'å¤šè§’åº¦åˆ†æ',
                'é“¾å¼å¤„ç†',
                'æ™ºèƒ½æ¨¡å‹é€‰æ‹©',
                'é”™è¯¯æ¢å¤'
            ]
        }